
В искусственных нейронных сетях функция активации нейрона определяет выходной сигнал, который определяется входным сигналом или набором входных сигналов. Стандартная компьютерная микросхема может рассматриваться как цифровая сеть функций активации, которые могут принимать значения «ON» (1) или «OFF» (0) в зависимости от входа. Это похоже на поведение линейного перцептрона в нейронных сетях. Однако только нелинейные функции активации позволяют таким сетям решать нетривиальные задачи с использованием малого числа узлов. В искусственных нейронных сетях эта функция также называется передаточной функцией.

В биологических нейронных сетях функция активации обычно является абстракцией, представляющей скорость возбуждения потенциала действия в клетке [1]. В наиболее простой форме эта функция является двоичной — то есть нейрон либо возбуждается, либо нет. Функция выглядит как 



ϕ
(

v

i


)
=
U
(

v

i


)


{\displaystyle \phi (v_{i})=U(v_{i})}

, где 



U


{\displaystyle U}

 — ступенчатая функция Хевисайда. В этом случае нужно использовать много нейронов для вычислений за пределами линейного разделения категорий.

Прямая с положительным угловым коэффициентом может быть использована для отражения увеличения скорости возбуждения по мере увеличения входного сигнала. Такая функция имела бы вид 



ϕ
(

v

i


)
=
μ

v

i




{\displaystyle \phi (v_{i})=\mu v_{i}}

, где 



μ


{\displaystyle \mu }

 — наклон прямой. Эта функция активации линейна, а потому имеет те же проблемы, что и двоичная функция. Кроме того, сети, построенные с использованием таковой модели, имеют нестабильную сходимость[en], поскольку возбуждение приоритетных входов нейронов стремится к безграничному увеличению, так как эта функция не нормализуема[en].

Все проблемы, упомянутые выше, можно решить с помощью нормализуемой сигмоидной функции активации. Одна из реалистичных моделей остаётся в нулевом состоянии, пока не придёт входной сигнал, в этот момент скорость возбуждения сначала быстро возрастает, но постепенно достигает асимптоты в 100 % скорости возбуждения. Математически, это выглядит как 



ϕ
(

v

i


)
=
U
(

v

i


)

t
h


(

v

i


)


{\displaystyle \phi (v_{i})=U(v_{i})\mathrm {th} \,(v_{i})}

, где гиперболический тангенс можно заменить любой сигмоидой. Такое поведение реально отражается в нейроне, поскольку нейроны не могут физически возбуждаться быстрее некоторой определённой скорости. Эта модель имеет несколько проблем, однако в вычислительных сетях, поскольку функция не дифференцируема, что нужно для вычисления обратной передачи ошибки обучения.

Последняя модель, которая используется в многослойных перцептронах — сигмоидная функция активации в форме гиперболического тангенса. Обычно используются два вида этой функции: 



ϕ
(

v

i


)
=

t
h


(

v

i


)


{\displaystyle \phi (v_{i})=\mathrm {th} \,(v_{i})}

, образ которой нормализован к интервалу [-1, 1], и 



ϕ
(

v

i


)
=
(
1
+
exp
⁡
(
−

v

i


)

)

−
1




{\displaystyle \phi (v_{i})=(1+\exp(-v_{i}))^{-1}}

, сдвинутая по вертикали для нормализации от 0 до 1. Последняя модель считается более биологически реалистичной, но имеет теоретические и экспериментальные трудности с вычислительными ошибками некоторых типов.

Специальный класс функций активации, известный как радиальные базисные функции (РБФ) используются в РБФ сетях, которые крайне эффективно в качестве универсальных аппроксиматоров функций. Эти функции активации могут принимать множество форм, но обычно берётся одна из следующих трёх функций:

где 




c

i




{\displaystyle c_{i}}

 является вектором, представляющим центр функции, а 



a


{\displaystyle a}

 и 



σ


{\displaystyle \sigma }

 являются параметрами, влияющими на расходимость радиуса.

Методы опорных векторов (SVM) могут эффективно использовать класс функций активации, который включает как сигмоиды, так и РБФ. В этом случае вход преобразуется для отражения гиперплоскости границы решений основываясь на нескольких обучающих входных данных, называемых опорными векторами 



x


{\displaystyle x}

. О функции активации для закрытого уровня этих машин говорят как о ядре скалярного произведения (англ. inner product kernel), 



K
(

v

i


,
x
)
=
ϕ
(

v

i


)


{\displaystyle K(v_{i},x)=\phi (v_{i})}

. Опорные вектора представляются как центры в РБФ с ядром, равным функции активации, но они принимают единственный вид в перцептроне 

где для сходимости 




β

0




{\displaystyle \beta _{0}}

 и 




β

1




{\displaystyle \beta _{1}}

 должны удовлетворять некоторым условиям. Эти машины могут принимать полиномиальные функции активации любого порядка

Функции активации бывают следующих типов:

Некоторые желательные свойства функция активации:

Следующая таблица сравнивает свойства некоторых функций активации, которые являются функциями одной свёртки x от предыдущего уровня или уровней:

(англ. Rectified linear unit, ReLU) [12][13]

с 



λ
=
1
,
0507


{\displaystyle \lambda =1,0507}

 и 



α
=
1
,
67326


{\displaystyle \alpha =1,67326}



Следующая таблица перечисляет функции активации, которые не являются функциями от одной свёртки x от предыдущего уровня или уровней:

↑  Здесь 




δ

i
j




{\displaystyle \delta _{ij}}

 обозначает символ Кронекера.

